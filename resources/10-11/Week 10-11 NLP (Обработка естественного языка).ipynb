{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b220774",
   "metadata": {},
   "source": [
    "# Общие сведения об NLP\n",
    "До этого мы построили с помощью **Scikit-Learn** модели машинного обучения, например, [Классификация текста - Анализ эмоциональной окраски](http://localhost:8888/notebooks/Documents/Course%20Data%20Science/1_Applied_ML_Course/Chapter%204/7_1_Sentiment%20Analysis.ipynb). Но сегодня классификация текстов выполняется с помощью *нейронных сетей*, т.е. с использованием модели глубокого обучения.\n",
    "\n",
    "**NLP** включает в себя различные виды задач:\n",
    "- классификацию текстов\n",
    "- распознавание именованных сущностей\n",
    "- извлечение ключевых слов\n",
    "- ответы на вопросы \n",
    "- перевод на другой язык и т.д. \n",
    "\n",
    "В последние годы точность таких моделей повысилась благодаря плотным векторным представлениям, трансформеру (transfonner), позволяющая выделять наиболее значимые слова и даже различать разные значения одного и того же слова.\n",
    "\n",
    "Одним из общих элементов практически всех обрабатывающих текст нейронных сетей является слой **Embedding**. \n",
    "\n",
    "## Подготовка текста\n",
    "\n",
    "Класс CountVectorizer от Scikit-Learn, преобразует строки текста в строки количества слов, преобразует символы в строчные, удаляет цифры и знаки препинания и, удаляет стопслова - такие распространенные слова, как and и the, которые, скорее всего, не\n",
    "окажут большого влияния на результат.\n",
    "\n",
    "В нейросети с глубоким обучением текст также должен быть очищен и векторизован перед использованием в обучении нейронной сети, но векторизация выполняется по-другому. Вместо того чтобы создавать таблицу количества слов, создается таблица последовательностей, содержащих представляющие отдельные слова токены. Токены часто являются индексами словаря, или лексикона, созданного на основе совокупности слов в наборе данных. Для этого в Keras имеется класс Tokenizer, которую можно рассматривать как эквивалент CountVectorizer\n",
    "\n",
    "Вот пример, в котором Tokenizer используется для создания последовательностей из четырех строк текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a67168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "lines = [\n",
    "'The quick brown fox',\n",
    "'Jumps over $$$ the lazy brown dog',\n",
    "    'Who jumps high into the blue sky after counting 123',\n",
    "'And quickly returns to earth'\n",
    "]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31102c",
   "metadata": {},
   "source": [
    "Метод *fit_on_texts()* создает словарь, содержащий все слова входного текста. Метод *texts_to_sequences* возвращает список\n",
    "последовательностей, являющихся просто массивами индексов в словаре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26941cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'brown': 2,\n",
       " 'jumps': 3,\n",
       " 'quick': 4,\n",
       " 'fox': 5,\n",
       " 'over': 6,\n",
       " 'lazy': 7,\n",
       " 'dog': 8,\n",
       " 'who': 9,\n",
       " 'high': 10,\n",
       " 'into': 11,\n",
       " 'blue': 12,\n",
       " 'sky': 13,\n",
       " 'after': 14,\n",
       " 'counting': 15,\n",
       " '123': 16,\n",
       " 'and': 17,\n",
       " 'quickly': 18,\n",
       " 'returns': 19,\n",
       " 'to': 20,\n",
       " 'earth': 21}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627215d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 2, 5], [3, 6, 1, 7, 2, 8], [9, 3, 10, 11, 1, 12, 13, 14, 15, 16], [17, 18, 19, 20, 21]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d1fe7",
   "metadata": {},
   "source": [
    "Слово brown встречается в строках 1 и 2 и представлено индексом 2. Следовательно, 2 встречается в обеих последовательностях. Аналогично, индекс 3, обозначающий слово jumps, встречается в последовательностях 2 и 3. Индекс О не используется для обозначения слов; он зарезервирован. \n",
    "\n",
    "Можно использовать метод sequences_to_texts класса Tokenizer для обратного преобразования последовательностей в текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b71a912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the quick brown fox', 'jumps over the lazy brown dog', 'who jumps high into the blue sky after counting 123', 'and quickly returns to earth']\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.sequences_to_texts(sequences)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be037985",
   "metadata": {},
   "source": [
    "Как видно Tokenizer преобразует текст в строчные буквы и удаляет символы, но не удаляет стоп-слова или числа. Если нужно удалить стоп-слова, то можно использовать отдельную библиотеку, например NLTK. Также можно удалить слова, содержащие цифры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2035e686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 4], [2, 5, 1, 6], [2, 7, 8, 9, 10], [11, 12, 13]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "lines = [\n",
    "'The quick brown fox',\n",
    "'Jumps over $$$ the lazy brown dog',\n",
    "'Who jumps high into the blue sky after counting 123',\n",
    "'And quickly returns to earth'\n",
    "]\n",
    "def remove_stop_words(text):\n",
    "    text = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word.isalpha() and not word in stop_words]\n",
    "    return ' '.join(text)\n",
    "lines = list(map(remove_stop_words, lines))\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7a0d1",
   "metadata": {},
   "source": [
    "Преобразуем теперь в текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29691878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick brown fox', 'jumps lazy brown dog', 'jumps high blue sky counting', 'quickly returns earth']\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.sequences_to_texts(tokenizer.texts_to_sequences(lines))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e572c82",
   "metadata": {},
   "source": [
    "Как видите длина последовательностей первой строки - 3, второй строки - 4, третьей - 5 и последней - 3. Т.е. длина последовательностей варьируется от 3 до 5, но наша нейронная сеть ожидает, что все последовательности будут *одинаковой длины*. Поэтому с использованием функции Keras pad_sequences, обрезаем последовательности, длина которых больше указанной, и заполняя нулями последовательности, длина которых меньше указанной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fe7528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  2,  5],\n",
       "       [ 1,  7,  2,  8],\n",
       "       [13, 14, 15, 16],\n",
       "       [18, 19, 20, 21]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=4)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fcb69",
   "metadata": {},
   "source": [
    "После того как текст обработан и преобразован в **последовательности**, он готов для обучения нейронной сети. \n",
    "\n",
    "Теперь задача слоя Embedding состоит в этом:\n",
    "- преобразовать заполненные последовательности словарных токенов в массивы векторов слов, представляющих каждое слово не одним целым числом, а массивом (вектором) чисел с плавающей точкой. \n",
    "\n",
    "Каждое слово во входном тексте представлено вектором в слое Embedding, и по мере обучения сети векторы, представляющие отдельные слова, корректируются, чтобы можно было отразить их связь друг с другом. \n",
    "\n",
    "Например, в модели анализа настроений и такие слова, как \"превосходный\" и \"потрясающий\", имеют схожий смысл, поэтому векторы, представляющие эти слова в пространстве Embedding, должны быть расположены относительно близко друг к другу, чтобы фразы типа \"превосходное обслуживание\" и \"потрясающее обслуживание\" оценивались одинаково.\n",
    "\n",
    "Реализация слоя Embedding вручную - сложная задача, поэтому Keras предлагает класс Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4031ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding(input_dim=10000, output_dim=32, input_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703849a6",
   "metadata": {},
   "source": [
    "В функцию Embedding передаются три параметра по следующему порядку:\n",
    "\n",
    "♦ размер словаря, или количество слов в словаре, созданном Tokenizer;\n",
    "\n",
    "♦ количество измерений m в эмбеддинговом пространстве;\n",
    "\n",
    "♦ длина n каждой заполненной последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5441c8",
   "metadata": {},
   "source": [
    "Вы выбираете число измерений m, и каждое слово кодируется в эмбеддинговом пространстве как m-мерный вектор. Большее число измерений обеспечивает большую мощность подгонки, но также увеличивает время обучения. На практике m обычно представляет собой число от 32 до 512.\n",
    "\n",
    "Векторы, представляющие отдельные слова в эмбеддинговом слое, изучаются в процессе обучения, так же как изучаются веса, соединяющие нейроны в соседних плотных слоях. Если количество обучающих образцов достаточно велико, при обучении сети обычно создаются эффективные векторные представления всех слов. Однако если у вас всего несколько сотен обучающих образцов, эмбединговый слой может не обладать информацией в достаточном количестве для правильной векторизации текста.\n",
    "\n",
    "В этом случае вы можете инициализировать эмбеддинговый слой предварительно обученными эмбеддингами слов, а не полагаться на то, что он сам освоит эмбеддинги слов. В открытом доступе существует несколько популярных предварительно обученных эмбеддингов слов, например, разработанные в Стэнфорде векторы слов GloVe и разработку Word2Vec от Google. \n",
    "\n",
    "Пример применения предварительно обученных эмбеддингов приведен в статье \"Использование предварительно обученных эмбеддингов\n",
    "слов\" [\"Using Pretrained Word Embeddings\"](https://keras.io/examples/nlp/pretrained_word_embeddings/), написанной автором Keras.\n",
    "\n",
    "Теперь на примере классификации текста посмотрим это.\n",
    "\n",
    "Построим архитектуру нейронной сети, выполняющей классификацию текста. Токенизированные текстовые последовательности поступают на вход эмбеддингового слоя. Выходом эмбеддингового слоя является двумерная матрица значений с плавающей точкой размером m на n, где m - количество измерений в эмбеддинговом пространстве, а n - длина последовательности. Слой Flatten, следующий за эмбеддинговым слоем, \"разглаживает\" двумерный выход в одномерный массив, пригодный для ввода в плотный слой, а плотный слой классифицирует значения, полученные из слоя Flatten. \n",
    "\n",
    "<img src=\"1.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5e10eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=100))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf379e53",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим его применение к задаче [фильтрации спама](http://localhost:8888/notebooks/Documents/Course%20Data%20Science/1_Applied_ML_Course/Chapter%2013/Spam%20Detection.ipynb). До этого с помощью Scikit построили модель машинного обучения, отделяющую спам от нормальных писем. Теперь построим эквивалентную модель глубокого обучения с помощью Keras и TensorFlow. Будем использовать тот же набор данных, что и раньше: набор, содержащий 1000 писем, половина из которых является спамом (обозначается единицами в столбце меток), а половина- нет (обозначается нулями в столбце меток).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb17c0c",
   "metadata": {},
   "source": [
    "**Д/З** Написать комментарий к каждой строке в задаче \"Hugging Face задача.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
