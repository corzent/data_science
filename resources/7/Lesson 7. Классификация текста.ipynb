{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d32452c",
   "metadata": {},
   "source": [
    "# Классификация текста\n",
    "Одним из наиболее распространенных применений бинарной классификации является анализ эмоциональной окраски, например отзыв о товаре, комментарий, оставленный на сайте, оценивается по шкале от 0,0 до 1,0, где 0,0 - негативность, а 1,0 - позитивность. Такой отзыв, как \"отличный продукт по отличной цене\", может получить 0,9 балла, а \"продукт по завышенной цене, который почти не работает\" - 0,1 балла. Балл - это вероятность того, что текст выражает положительное настроение. \n",
    "\n",
    "Анализ эмоциональной окраски - это один из примеров задачи, в которой классифицируются текстовые, а не числовые данные. Поскольку машинное обучение работает с числами, перед обучением модели спам, или любой текст, необходимо преобразовать в числа. Распространенным подходом является построение таблицы частот слов, называемой мешок слов (bag of words).  В библиотеке Scikit-Learn имеются классы для преобразования, для нормализации текста и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7fd9e",
   "metadata": {},
   "source": [
    "# Подготовка текста к классификации\n",
    "Прежде чем обучать модель классификации текста, необходимо преобразовать текст в числа - этот процесс называется **векторизацuей**. До этого была приведена пример, демонстрирующая распространенную методику векторизации текста. Каждая строка представляет собой образец текста, например, рецензию на фильм, а каждый столбец - слово в обучающем тексте. Числа в строках - это количество слов, а последнее число в каждой строке - это метка: 0 - отрицательная, 1 - положительная."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b881e",
   "metadata": {},
   "source": [
    "<img src=\"Data/Complex dataset.JPG\" align='left'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b36b46",
   "metadata": {},
   "source": [
    "Перед векторизацией текст обычно подвергается очистке. В качестве примеров очистки можно привести преобразование символов в строчные (например, Excellent будет эквивалентно excellent), удаление знаков препинания и, по желанию, удаление стоп-слов - таких распространенных слов, как the и and, которые, скорее всего, не окажут существенного влияния на результат. После очистки предложения разбиваются на отдельные слова (токенизируются), и эти слова используются для создания наборов данных.\n",
    "\n",
    "В Scikit-Learn есть три класса, которые выполняют основную часть работы по очистке и векторизации текста.\n",
    "\n",
    "- CountVectorizer -cоздает словарь (vocabulary) из массива слов в обучающем тексте и формирует\n",
    "матрицу количества слов, подобную выше.\n",
    "- HashingVectorizer - использует хеши слов, а не словарь в памяти для получения количества слов и,\n",
    "следовательно, более экономичен с точки зрения памяти.\n",
    "- TfidfVectorizer - создает словарь из предоставленных слов и формирует матрицу, аналогичную\n",
    "приведенной на рисунке выше, но вместо целочисленного количества слов матрица содержит значения частоты терминов и обратной частоты документов (term frequency, inverse document frequency, TFIDF) от 0,0 до 1,0 отражающие относительную важность отдельных слов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344ae88",
   "metadata": {},
   "source": [
    "Все три класса способны преобразовывать текст в строчные буквы, удалять знаки препинания, удалять стоп-слова, разбивать предложения на отдельные слова и т. д.\n",
    "Они также поддерживают n-граммы, представляющие собой комбинации двух или более последовательных слов (число п задается пользователем), которые должны рассматриваться как одно слово. Идея заключается в том, что такие слова, как credit и score, могут быть более значимыми, если они встречаются в предложении рядом друг с другом, чем если они находятся далеко друг от друга. \n",
    "\n",
    "Нейронные сети имеют другие более мощные способы учета порядка слов, которые не требуют, что связанные слова находились рядом с друг другом. Обычная модель машинного обучения не может связать слова \"синий\" и \"небо\" в предложении \"Мне нравиться синий, потому что это цвет неба\", а нейронная сеть сможет. \n",
    "\n",
    "**Д/з** конспектировать нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f3aba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ago</th>\n",
       "      <th>brought</th>\n",
       "      <th>conceived</th>\n",
       "      <th>created</th>\n",
       "      <th>dedicated</th>\n",
       "      <th>equal</th>\n",
       "      <th>equals</th>\n",
       "      <th>fathers</th>\n",
       "      <th>forth</th>\n",
       "      <th>freedom</th>\n",
       "      <th>fог</th>\n",
       "      <th>liberty</th>\n",
       "      <th>mеn</th>\n",
       "      <th>nation</th>\n",
       "      <th>new</th>\n",
       "      <th>proposltlon</th>\n",
       "      <th>score</th>\n",
       "      <th>years</th>\n",
       "      <th>аге</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Line 1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line 3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line 4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ago  brought  conceived  created  dedicated  equal  equals  fathers  \\\n",
       "Line 1    1        1          0        0          0      0       0        1   \n",
       "Line 2    0        0          1        0          0      0       0        0   \n",
       "Line 3    0        0          0        1          1      1       0        0   \n",
       "Line 4    0        0          0        0          0      0       1        0   \n",
       "\n",
       "        forth  freedom  fог  liberty  mеn  nation  new  proposltlon  score  \\\n",
       "Line 1      1        0    0        0    0       0    0            0      1   \n",
       "Line 2      0        0    0        1    0       1    1            0      0   \n",
       "Line 3      0        0    0        0    1       0    0            1      0   \n",
       "Line 4      0        2    1        0    0       2    0            0      0   \n",
       "\n",
       "        years  аге  \n",
       "Line 1      1    0  \n",
       "Line 2      0    0  \n",
       "Line 3      0    1  \n",
       "Line 4      0    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "lines = [\n",
    "'Four score and 7 years ago our fathers brought forth,',\n",
    "' ... а new NATION, conceived in liberty $$$,',\n",
    "'and dedicated to the PrOpOsltloN that all mеn аге created equal',\n",
    "'One nation\\'s freedom equals #freedom fог another $nation!'\n",
    "]\n",
    "# Осуществляем векторизацию\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "word_matrix = vectorizer.fit_transform(lines)\n",
    "# Показываем резулЬl'luру1а4Ую ма~рицу слов\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "line_names = [f'Line {(i + 1):d}' for i, _ in enumerate(word_matrix)]\n",
    "df = pd.DataFrame(data=word_matrix.toarray(), index=line_names, columns=feature_names)\n",
    "df .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9daaa05",
   "metadata": {},
   "source": [
    "В данном случае блок текста представляет собой четыре строки в списке языка Python. Класс CountVectoгizer разбил строки на слова, удалил стоп-слова и символы, а все оставшиеся слова перевел в нижний регистр. Эти слова составляют столбцы набора данных, а числа в строках показывают, сколько раз данное слово встречается в каждой строке. Параметр stop_words=' english' указывает CountVectoгizer на удаление стоп-слов с помощью встроенного словаря, содержащего более 300 англоязычных стоп-слов. При желании можно предоставить собственный список стопслов в виде списка на языке Python. (Или можно оставить стоп-слова; часто это не имеет значения.) А если вы работаете с текстом, написанным на другом языке, списки мультиязычных стоп-слов можно получить из других библиотек Python, таких  как Natural Language Toolkit (NLТК) и Stop-words.\n",
    "\n",
    "Из вывода видно, что слова equal и equals считаются отдельными словами, несмотря на то что они имеют схожее значение. При подготовке текста к машинному обучению специалисты по исследованию данных иногда идут еще дальше, выполняя стемминг или лемматизацию слов.\n",
    "Если бы предыдущий текст был стемминговым, то все вхождения equals были бы преобразованы в equal. В Scikit отсутствует поддержка стемминга и лемматизации, но ее можно получить из других библиотек, например NLТК.\n",
    "\n",
    "Класс CountVectoгizer удаляет знаки препинания, но не удаляет цифры. Он пропустил 7 в строке 1, поскольку игнорирует одиночные символы. Но если заменить 7 на 777, то в словаре появится термин 777. Один из способов исправить это - определить функцию, удаляющую цифры, и передать ее в CountVectoгizer через параметр preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79813aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install regex\n",
    "import re\n",
    "def preprocess_text(text):\n",
    "    return re.sub(r'\\d+', '', text).lower()\n",
    "vectorizer = CountVectorizer(stop_words='english', preprocessor=preprocess_text)\n",
    "word_matrix = vectorizer.fit_transform(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd28064",
   "metadata": {},
   "source": [
    "Обратите внимание на вызов lower для преобразования текста в нижний регистр. CountVectorizer не выполняет такое преобразование, если вы используете функцию предварительной обработки, поэтому указанная функция должна преобразовать данные самостоятельно. Однако она все равно удаляет знаки препинания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aeca0c",
   "metadata": {},
   "source": [
    "Еще одним полезным параметром CountVectorizer является min_df, который позволяет игнорировать слова, встречающиеся меньше указанного количества раз. Это может быть целое число, задающее минимальное количество (например, игнорировать слова, встречающиеся в обучающем тексте менее пяти раз, min.n_df=S), или значение с плавающей точкой от 0,0 до 1,0, задающее минимальный процент образцов, в которых должно встречаться слово (например, игнорировать слова, встречающиеся менее\n",
    "чем в 10% образцов, minn_df=0,1). Это позволяет отфильтровать слова, которые, вероятно, все равно не имеют смысла, а также сократить потребление памяти и время обучения за счет уменьшения размера словаря. CountVectorizer также поддерживает параметр max_df для исключения слов, которые встречаются слишком часто.\n",
    "В предыдущих примерах использовался класс CountVectorizer, что, вероятно, заставило вас задуматься о том, когда (и почему) вместо него можно применить HashingVectorizer или TfidfVectorizer. Класс HashingVectorizer полезен при работе с большими наборами данных. Вместо того, чтобы хранить слова в памяти, он хеширует каждое слово и использует хеш в качестве индекса в массиве количества слов.\n",
    "Таким образом, он может делать больше, используя меньше памяти, и очень полезен для уменьшения размера векторизаторов при их сериализации с целью последующего восстановления. Недостатком\n",
    "HashingVectorizer является то, что он не разрешает работать в обратном направлении от векторизованного текста к исходному. А вот CountVectorizer позволяет, и для этого в нем предусмотрен метод inverse_transforlm.\n",
    "\n",
    "ТfidfVectorizer часто используется для извлечения ключевых слов: документ или набор документов изучаются, и извлекаются ключевые слова, характеризующие их содержание. При этом словам присваиваются числовые веса, отражающие их важность, а для определения весов используются два фактора: частота появления слова в отдельных документах и частота его появления в общем наборе документов. Слова, которые чаще встречаются в отдельных документах, но в меньшем количестве документов, получают более высокие веса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c57b6",
   "metadata": {},
   "source": [
    "<font color=\"#9900FF\">  After showing *Sentiment Analysis* explain Naive Bayes algorithm. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972faa07",
   "metadata": {},
   "source": [
    "# Наивный Байес\n",
    "Логистическая регрессия является основным алгоритмом для моделей классификации и часто очень эффективна при классификации текста. Однако, специалисты по обработке данных часто обращаются к другому алгоритму обучения, называемому \"наивный Байес\" (Naive Bayes). Это алгоритм классификации, основанный на теореме Байеса, которая позволяет вычислять условные вероятности. Математически теорема Байеса выглядит следующим образом:\n",
    "\n",
    "$Р(A|B)-\\frac{P(B|A)*P(A)}{Р(В)}$\n",
    "\n",
    "Вероятность того, что А истинно при условии, что В истинно, равна вероятности того, что В истинно при условии, что А истинно, умноженной на вероятность того, что А истинно, деленной на вероятность того, что В истинно. \n",
    "\n",
    "Попробуем применить его например, к коллекции электронных писем, чтобы определить, какие из них являются спамом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff337b",
   "metadata": {},
   "source": [
    "Предположим, что 10% всех полученных вами писем являются спамом. Это Р(А). Анализ показывает, что 5% полученных вами спам писем содержат слово congratulations (поздравления), но только 1 % всех ваших писем содержат то же самое слово. Таким образом, Р(В|A) составляет 0,05, а Р(В)- 0,01. Вероятность того, что письмо является спамом, если оно содержит слово congratulations, равна Р(А|В), что составляет ($0,05*0,10) / 0,01$, или 0,50.\n",
    "Разумеется, спам-фильтр должен учитывать все слова в письме, а не только одно. Оказывается, если сделать несколько простых (наивных) предположений о том, что порядок слов в письме не имеет значения и каждое слово имеет одинаковый вес, то уравнение Байеса для классификатора спама можно записать таким образом:\n",
    "\n",
    "$P(S|сообщение)= P(S)·P(слово_1|S)-P(слово_2|S)-··P(слово_n|S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be3d70",
   "metadata": {},
   "source": [
    "Вероятность того, что сообщение является спамом, пропорциональна произведению:\n",
    "- вероятности того, что любое сообщение в наборе данных является спамом, или P(S);\n",
    "- вероятности того, что каждое слово в сообщении встречается в спаме, или Р(слово|S).\n",
    "\n",
    "P(S) - это доля сообщений в наборе данных, которые являются спамом. Если вы обучаете МL-модель на 1000 сообщений и 500 из них являются спамом, то P(S) = 0,5. \n",
    "\n",
    "Для заданного слова Р(слово|S)- это просто количество раз, которое это слово встречается в спам письмах, поделенное на количество слов во всех спам письмах. Вся проблема сводится к подсчету\n",
    "количества слов. Аналогичным образом можно вычислить вероятность того, что сообщение не является спамом, а затем использовать большую из двух вероятностей для прогноза."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e075b",
   "metadata": {},
   "source": [
    "<font color=\"#9900FF\"> Next, show the Spam detection and go back. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e80c8",
   "metadata": {},
   "source": [
    "# Рекомендательные системы\n",
    "Рекомендательные системы - системы, рекомендующие покупателям товары или услуги. Рекомендательные системы бывают разных видов. \n",
    "- Системы, основанные на популярности, предлагают покупателям варианты, основанные на том, какие товары и услуги популярны в данный момент, например \"Бестселлеры этой недели\". \n",
    "- Коллаборативные системы дают рекомендации на основе того, что выбрали другие, например \"Люди, купившие эту книгу, также купили эти книги\". Ни одна из этих систем не требует машинного обучения.\n",
    "- Контентные системы. Примером системы, основанной на контенте, является система, которая говорит: \"Если вы купили эту книгу, то вам могут понравиться и эти книги\". Если Вам нравиться \"Крепкий орешек\", то вам может понравиться или не понравиться фильм \"Монти Пайтон и Священный Грааль\". \n",
    "\n",
    "Но как определить это алгоритмически?\n",
    "Для контентных рекомендательных систем необходимы два компонента: способ векторизации атрибутов, характеризующих услугу или товар, и способ вычисления сходства между полученными векторами. Первое несложно. CountVectortzer преобразует текст в вектор слов. Достаточно найти способ измерения сходства между строками количества слов, и можно строить рекомендательную систему. Одним из самых простых и эффективных способов является метод косинусного сходства.\n",
    "\n",
    "# Косинусное сходство\n",
    "Косинусное сходство - это математический способ вычисления сходства между парами векторов (или рядов чисел, рассматриваемых как векторы). Основная идея состоит в том, чтобы взять каждое значение в выборке, например количество слов в строке векторизованного текста, и использовать их в\n",
    "качестве координат конечной точки вектора, причем другая конечная точка должна находиться в начале системы координат. Проделайте это для двух выборок, а затем вычислите косинус между векторами в m-мерном пространстве, где m - количество значений в каждой выборке. Поскольку косинус 0 равен 1, два одинаковых вектора имеют сходство, равное 1. Чем более несхожи векторы, тем ближе косинус к 0.\n",
    "\n",
    "В качестве иллюстрации приведем пример в двумерном пространстве. Предположим, что у вас есть три строки, содержащие по два значения:\n",
    "\n",
    "    1     2\n",
    "\n",
    "    2     3\n",
    "\n",
    "    3     1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587dae4",
   "metadata": {},
   "source": [
    "Требуется определить, на что больше похож ряд 2 - на ряд 1 или на ряд 3? Это трудно сказать, просто взглянув на числа, а в реальной жизни таких чисел бывает гораздо больше. Если просто сложить числа в каждом ряду и сравнить суммы, то можно сделать вывод, что ряд 2 более похож на ряд 3. Но что если рассматривать каждый ряд как вектор?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb67cfd",
   "metadata": {},
   "source": [
    "<img src=\"RecommenderSystems.png\" width=600 height=600 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e5b73",
   "metadata": {},
   "source": [
    "- Row 1: (0, 0) → (1, 2)\n",
    "- Row 2: (0, 0) → (2, 3)\n",
    "- Row 3: (0, 0) → (3, 1)\n",
    "\n",
    "Теперь можно представить каждый ряд в виде вектора, вычислить косинусы углов, образованных прямыми 1 и 2, 2 и 3, и определить, что ряд 2 больше похож на ряд 1, чем на ряд 3. Это и есть косинусное сходство в общих чертах.\n",
    "\n",
    "Косинусное сходство не ограничивается двумя измерениями, оно работает и в многомерном пространстве. \n",
    "\n",
    "Для вычисления косинусного сходства независимо от числа измерений Scikit предлагает функцию cosine_similaгity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df08145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.99227788, 0.70710678],\n",
       "       [0.99227788, 1.        , 0.78935222],\n",
       "       [0.70710678, 0.78935222, 1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data = [[1, 2], [2, 3], [3, 1]]\n",
    "cosine_similarity(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d182e",
   "metadata": {},
   "source": [
    "Возвращаемое значение - матрица подобия (similarity matrix) содержит косинусы каждой пары векторов. Ширина и высота матрицы равны количеству образцов. \n",
    "\n",
    "Отсюда видно, что сходство строк 1 и 2 составляет 0,992, а сходство строк 2 и 3 - 0,789. Другими словами, ряд 2 больше похож на ряд 1, чем на ряд 3. Сходство между рядами 2 и 3 (0,789) также больше, чем между рядами 1 и 3 (0,707)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4eeba",
   "metadata": {},
   "source": [
    "# Построение системы рекомендаций фильмов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32915279",
   "metadata": {},
   "source": [
    "<font color=\"#9900FF\"> Go to Movie Recommendations </font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
